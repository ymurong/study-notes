- [Content of Table](#content-of-table)
  - [Ranking](#ranking)
    - [Query analysis pipeline](#query-analysis-pipeline)
    - [Index Types](#index-types)
      - [Inverted index](#inverted-index)
      - [Approximate nearest nerighbor (ANN) Index](#approximate-nearest-nerighbor-ann-index)
      - [Forward Index](#forward-index)
    - [Ranking pipeline](#ranking-pipeline)
  - [Term based retrieval](#term-based-retrieval)
    - [Vector Space Model](#vector-space-model)
    - [Language Modeling](#language-modeling)
      - [Types](#types)
      - [Documents as distributions](#documents-as-distributions)
      - [Query likelihood model](#query-likelihood-model)
      - [Smoothing](#smoothing)
    - [BM25](#bm25)
  - [Semantic based Retrieval](#semantic-based-retrieval)
    - [Topic modeling (LDA)](#topic-modeling-lda)
      - [Mixture of unigrams](#mixture-of-unigrams)
      - [pLSA(Probabilistic latent semantic analysis)](#plsaprobabilistic-latent-semantic-analysis)
      - [LDA(Latent Dirichlet allocation)](#ldalatent-dirichlet-allocation)
      - [LDA estimation](#lda-estimation)
    - [Latent semantic indexing/analysis (LSA)](#latent-semantic-indexinganalysis-lsa)
      - [SVD(singular value decomposition)](#svdsingular-value-decomposition)
      - [Document as vector](#document-as-vector)
  - [Neural Models](#neural-models)
    - [Word embeddings (average)](#word-embeddings-average)
    - [Document embeddings](#document-embeddings)


# Content of Table
## Ranking

### Query analysis pipeline 
- should be the same as document analysis


### Index Types
#### Inverted index
- given a term, return document containing the term and associated data
#### Approximate nearest nerighbor (ANN) Index 
- given a vector query representation, return the (approximate) top-k nearest neighbors (top-k document IDs)
- First-stage semantic retrieval methods like LSI and neural dense retrieval
#### Forward Index 
- given a document ID, return the doc's text
- reranking methods like neural cross-encoders


### Ranking pipeline
- fast first-stage retrieval over all documents (usually BM25)
- slow second-stage reranking slower over a subset of documents (neural method)
- pruning -> filtering


## Term based retrieval

### Vector Space Model 
- TFIDF

### Language Modeling 
we treat documents as distribution of words

#### Types
- Unigram language model
- Bi-gram language model (order matters: rare to be used in first stage, not efficient

#### Documents as distributions
- A document is a multinomial distribution over words
- if some vocab terms do not appear in document d, then $P(t|M_{d}) = 0$
- addresed by smoothing

#### Query likelihood model
- $P(d|q) = \frac{P(q|d)P(d)}{P(q)}$
- ignore the prior distribution over query p(q)
  - $P(d|q) = P(q|d)P(d)$
- assume prior distribution over documents P(d) is assumed to be uniform
  - $P(d|q) = P(q|d) = P(q|M_{d})$ 
- Bag of words assumption (terms are independent)
  - $P(q|M_{d}) = \prod_{t\in{q}}P(t|M_{d})$ = $\prod_{t\in{q}}\frac{tf(t,d)}{dl(d)}$
  - dl(d) document length

#### Smoothing
- Jelinek-Mercer smoothing
- $P_{s}(t|M_{d}) = \lambda P(t|M_{d}) + (1-\lambda)P(t|M_{c})$
- = $\lambda\frac{tf(t,d)}{dl(d)} + (1-\lambda) \frac{cf(t)}{cl}$
- cl: collection length
- smoothing factor = $\lambda$ or $(1-\lambda)$ ???

### BM25 
- k1=(0, $\inf$) -> term saturation 
  - if k1 = 5, less important at 5, no importance after 6
  - usually between (1.2,2)
- b=(0,1) -> doc length normalization
  - if b=1, then we totally use doc length normalization as weight, otherwise, totally ignore doc length normalization
  - usually 0.75


## Semantic based Retrieval

### Topic modeling (LDA)
#### Mixture of unigrams
1. sample topic from given document distribution
   - $z_{i}  \sim Multi(\theta)$
2. sample word from given topic distribution
   - $w_{ij} \sim Multi(\phi_{z_{i}}))$

#### pLSA(Probabilistic latent semantic analysis)
- assume every word from document comes from a certain topic
#### LDA(Latent Dirichlet allocation)
- we add a prior distribution of documents over topics $\alpha$
- we add a prior distribution of topic over words $\beta$

#### LDA estimation
using expectation-maximization

### Latent semantic indexing/analysis (LSA)

#### SVD(singular value decomposition)
$$A = U \Sigma V^T$$
- U -> term representation
- $\Sigma$ -> singular value (in descending order)
- $V^T$ -> document representation

#### Document as vector
- Given a collection of documents, perform SVD and low-rank approximation to obtain $\Sigma_{k}$ and $U_{k}$.

- d and q represents the original sparse document/query representation composed by 0 and 1
- Then the new dense representation of document and query can be obtained by
  - $\^{d} = \Sigma_{k}^{-1}U_{k}^{T}d$
  - $\^{q} = \Sigma_{k}^{-1}U_{k}^{T}q$ (computed on the fly)

- Match the obtained "semantic" vector representations $\^{d}$ and $\^{q}$ using cosine similarity


## Neural Models

### Word embeddings (average)

- compute word embeddings (CBOW, Skip-gram)
  - [skip-gram in pytoch tutorial](https://blog.csdn.net/qq_24668285/article/details/121754529)
- given a document and a query, compute their vector representations as average word embeddings (AWEs)
- Match using cosine similarity

### Document embeddings 

- compute paragraph (document) vector (offline)
  - at every step of stochastic gradient descent, sample a fixed-length context (used as positive target) from a random paragraph (document)
  - compute the error gradient from the network
  - use the gradient to update parameters (D-matrix)


- when query arrives
  - already have the word matrix W and document matrix D
  - add a (randomly initialized) column (query) to the document matrix D
  - update D using gradient descent
  - get the vector representation of the query from the updated matrix D

- match using cosine similarity (D @ Q)







 