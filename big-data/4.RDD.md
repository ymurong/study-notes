
- [Table of Contents](#table-of-contents)
  - [Limits Map Reduce](#limits-map-reduce)
  - [Gradient Descent in MapReduce](#gradient-descent-in-mapreduce)
  - [RDD](#rdd)
    - [Description](#description)
    - [Characteristics](#characteristics)
    - [Transformations](#transformations)
    - [Actions](#actions)
    - [Narrow Dependencies](#narrow-dependencies)
    - [Wide Dependencies](#wide-dependencies)
  - [Spark DataFrame API](#spark-dataframe-api)
    - [Repartitioning](#repartitioning)
    - [Tips](#tips)

# Table of Contents

- Explain the benefits of the RDD (Resilient Distributed Datasets) abstraction over MapReduce
- Develop custom Spark programs for data tasks
- Classify the individual steps in a Spark program
- Assess a given Spark program for correctness and performance



## Limits Map Reduce 
only suitable for one-time job and simple dependencies

## Gradient Descent in MapReduce
- Reduce can only start once all mappers are 
done => high latency!
- Model parameters have to be broadcast after update
- wasteful intermediate steps

## RDD
### Description
- Core abstraction of the distributed data processing engine Apache Spark
- Motivated by growing need to efficiently execute applications that re-use intermediate results multiple across multiple operations 
- Read-only, fault-tolerant, partitioned, parallel data structure
  - The fault-tolerant as they can track data lineage information to allow for rebuilding lost data automatically on failure.
- System automatically handles parallelisation, work distribution  and fault tolerance

### Characteristics
- Created from a data source
- system records lineage graph of transformations applied to data
- operations executed lazily (deferred computations)


### Transformations

- Transformations turn one or more RDDs 
into a new RDD 
- Transformations are cheap to construct  
because they don’t actually execute the 
computation 
- Building an RDD is like writing (not 
running) a MapReduce script or a  
SQL query 
- map, filter, union
 
### Actions
- Actions execute the computations 
defined by an RDD 
- The results of actions are not RDDs ! 
- count, collect, reduce, save

### Narrow Dependencies 
Each partition of a parent RDD  contributes to at most one  partition of the child RDD 
- Low communication 
- No shuffling required 
- Pipelineable 
- Easy failure recovery
- map, filter, union, join with inputs co-partitioned


### Wide Dependencies
Each partition of parent RDD  contributes to multiple child  RDD partitions 
- Shuffling / repartitioning required 
- Pipeline breaker 
- Expensive to recover in  case of failures
- groupByKey, join with inputs not co-partitioned


## Spark DataFrame API
- DataFrames in Spark are like relations in an RDBMS 
  - Well-defined schema with types over columns 
  - Each row is a tuple 
- DataFrame operations are translated into RDD transformations by Spark 
- RDD transformations can then be executed within the JVM 
  - No more serialisation of data between JVM ⇔ Python!

### Repartitioning
- Sometimes you know in advance which columns of a DataFrame will be important for joins and aggregations 
- You might also know that data will be re-accessed 
- You can give hints to Spark how RDD partitions should be laid out and where they should be stored
  - This can accelerate many subsequent operations (given that there is enough memory in the cluster) 
- Similar to indexing in RDBMS

### Tips
- Before running an action, run the explain() method on the DataFrame 
  - This will give you an execution plan 
  - You might identify some inefficiencies or bugs this way 

- Be careful with collect() ! 
  -  Test-drive a large query with take(10) instead of collect()